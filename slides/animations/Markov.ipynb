{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "from manim import *\n",
    "from selma import BACKGROUND\n",
    "from selma.graph import MGraph, gvlayout_factory, test_draw\n",
    "\n",
    "config.background_color = BACKGROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChainTextGenerator:\n",
    "  def __init__(\n",
    "    self, order=2, token_type='word', smoothing_alpha=0.001, interpolation=True\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Markov Chain Text Generator with variable order and tokenization type.\n",
    "\n",
    "    Parameters:\n",
    "        order (int): Order of the Markov chain (n-gram size)\n",
    "        token_type (str): 'word' for word-level, 'char' for character-level\n",
    "        smoothing_alpha (float): Laplace smoothing factor\n",
    "        interpolation (bool): Use backoff interpolation for unseen sequences\n",
    "    \"\"\"\n",
    "    self.order = order\n",
    "    self.token_type = token_type\n",
    "    self.smoothing_alpha = smoothing_alpha\n",
    "    self.interpolation = interpolation\n",
    "    self.ngram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    self.total_counts = defaultdict(int)\n",
    "    self.vocab = set()\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    \"\"\"Tokenizes text into words or characters based on user preference.\"\"\"\n",
    "    if self.token_type == 'word':\n",
    "      return re.findall(r'\\b\\w+\\b', text.lower())  # Simple word tokenization\n",
    "    elif self.token_type == 'char':\n",
    "      return list(text.lower())  # Character-level tokenization\n",
    "    else:\n",
    "      raise ValueError(\"token_type must be 'word' or 'char'\")\n",
    "\n",
    "  def train(self, corpus):\n",
    "    \"\"\"Trains the Markov chain model on a given corpus.\"\"\"\n",
    "    tokens = self.tokenize(corpus)\n",
    "    self.vocab.update(tokens)\n",
    "    self.vocab.add('<UNK>')  # Handle unknown words\n",
    "\n",
    "    for i in range(len(tokens) - self.order):\n",
    "      prefix = tuple(tokens[i : i + self.order])  # n-gram prefix\n",
    "      next_token = tokens[i + self.order]  # next token\n",
    "\n",
    "      self.ngram_counts[prefix][next_token] += 1\n",
    "      self.total_counts[prefix] += 1\n",
    "\n",
    "  def get_next_token_probs(self, prefix):\n",
    "    \"\"\"\n",
    "    Retrieves transition probabilities using interpolation.\n",
    "    \"\"\"\n",
    "    prefix = tuple(prefix)\n",
    "    if prefix in self.ngram_counts:\n",
    "      total = self.total_counts[prefix] + self.smoothing_alpha * len(self.vocab)\n",
    "      return {\n",
    "        token: (count + self.smoothing_alpha) / total\n",
    "        for token, count in self.ngram_counts[prefix].items()\n",
    "      }\n",
    "\n",
    "    # Backoff: Try shorter prefixes\n",
    "    if self.interpolation:\n",
    "      for k in range(self.order - 1, 0, -1):\n",
    "        shorter_prefix = prefix[-k:]\n",
    "        if shorter_prefix in self.ngram_counts:\n",
    "          total = self.total_counts[shorter_prefix] + self.smoothing_alpha * len(\n",
    "            self.vocab\n",
    "          )\n",
    "          return {\n",
    "            token: (count + self.smoothing_alpha) / total\n",
    "            for token, count in self.ngram_counts[shorter_prefix].items()\n",
    "          }\n",
    "\n",
    "    # If no known transitions, return uniform probabilities over vocab\n",
    "    return {token: 1 / len(self.vocab) for token in self.vocab}\n",
    "\n",
    "  def generate(self, seed=None, length=50):\n",
    "    \"\"\"\n",
    "    Generates text based on the trained model.\n",
    "    \"\"\"\n",
    "    if not self.ngram_counts:\n",
    "      raise ValueError('Model is not trained. Call `train(corpus)` first.')\n",
    "\n",
    "    tokens = (\n",
    "      list(self.tokenize(seed))\n",
    "      if seed\n",
    "      else [random.choice(list(self.vocab - {'<UNK>'}))]\n",
    "    )\n",
    "\n",
    "    while len(tokens) < length:\n",
    "      prefix = tuple(tokens[-self.order :])\n",
    "      probs = self.get_next_token_probs(prefix)\n",
    "\n",
    "      next_token = random.choices(list(probs.keys()), weights=list(probs.values()))[0]\n",
    "      tokens.append(next_token)\n",
    "\n",
    "    return ' '.join(tokens) if self.token_type == 'word' else ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {\n",
    "  'mc': \"\"\"\n",
    "Markov chains are mathematical systems that undergo transitions from one state to another \n",
    "on a state space. A sequence of possible events in which the probability of each event \n",
    "depends only on the state attained in the previous event is called a Markov chain.\n",
    "\"\"\",\n",
    "  'ttls': \"\"\"\n",
    "Twinkle, twinkle, little star,\n",
    "How I wonder what you are!\n",
    "Up above the world so high,\n",
    "Like a diamond in the sky.\n",
    "\"\"\",\n",
    "  'gadda': 'Come poco pepe a coppia cuoce e scoppia',  # Gadda\n",
    "  'giovanni': 'In principio era il Verbo, e il Verbo era presso Dio, e il Verbo era Dio',  # dal Vangelo di Giovanni, 1:1\n",
    "  'commedia': Path('../../data/commedia.txt').read_text(),\n",
    "  'promessi': Path('../../data/papini.txt').read_text()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(corpus):\n",
    "  res = [' ']\n",
    "  for c in corpus:\n",
    "    l = c.lower()\n",
    "    if l.isalpha():\n",
    "      res.append(l)\n",
    "    elif l == ' ' and res[-1] != ' ':\n",
    "      res.append(' ')\n",
    "  return ''.join(res[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = 'giovanni'\n",
    "\n",
    "ORDER = 1\n",
    "generator = MarkovChainTextGenerator(\n",
    "  order=ORDER, token_type='word', smoothing_alpha=0, interpolation=False\n",
    ")\n",
    "generator.train(clean_text(corpora[CORPUS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dio e il verbo e il verbo e il verbo era dio e il verbo era dio e il verbo era il verbo era presso dio e il verbo era il verbo e il verbo era dio e il verbo era presso dio e il verbo era il verbo era presso dio e il verbo e il verbo era presso dio e il verbo e il verbo e il verbo era presso dio e il verbo era presso dio e il verbo era presso dio e il verbo era presso dio e il verbo era dio e il verbo era'"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('dio', length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_markov_chain(scene, corpus, by_char, layout, node_scale):\n",
    "  \n",
    "  if by_char:\n",
    "    data = [c for c in clean_text(corpus) if c != ' ']\n",
    "  else:\n",
    "    data = clean_text(corpus).split()\n",
    "\n",
    "  seq = list(zip(data, data[1:]))\n",
    "  G = nx.DiGraph(set(seq))\n",
    "  weight = {e: 0 for e in G.edges()}\n",
    "  G.remove_edges_from(nx.selfloop_edges(G))\n",
    "  \n",
    "  MG = MGraph(G, layout=layout, node_scale=node_scale)\n",
    "  \n",
    "  tt = [t for d in data for t in (d, ' ')]\n",
    "  T = Tex(*tt, color=BLACK).scale(.9)\n",
    "  T.to_edge(UL)\n",
    "  scene.add(T)\n",
    "  \n",
    "  def highlight(t):\n",
    "    mt = MG.mnode(t)\n",
    "    mt.z_index = 1\n",
    "    mt.set_stroke(color=PURE_GREEN)\n",
    "    T[highlight.step].set_color(PURE_GREEN)\n",
    "    scene.add(mt)\n",
    "    scene.wait(.5)\n",
    "    mt.set_stroke(color=DARK_BROWN)\n",
    "    T[highlight.step].set_color(BLACK)\n",
    "    highlight.step += 2\n",
    "  highlight.step = 0\n",
    "\n",
    "  highlight(seq[0][0])\n",
    "  for s, t in seq:\n",
    "    weight[(s, t)] += 1\n",
    "    if s != t:\n",
    "      me = MG.medge(s, t)\n",
    "      me.z_index = 0\n",
    "      me.set_stroke(width=weight[(s, t)] * 2, color=PURE_GREEN) \n",
    "      scene.add(me)\n",
    "    else:\n",
    "      me = None\n",
    "    highlight(t)\n",
    "    if me: me.set_stroke(color=BLACK) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"media/jupyter/MarkovByChar@2025-02-27@20-20-45.mp4\" controls autoplay loop style=\"max-width: 60%;\"  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%manim --hide-splash -qm -v WARNING MarkovByChar\n",
    "\n",
    "layout = \n",
    "\n",
    "class MarkovByChar(Scene):\n",
    "  def construct(self):\n",
    "    build_markov_chain(self, corpora['gadda'], by_char=True, layout = gvlayout_factory('dot', heightscale=.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"media/jupyter/MarkovByWord@2025-02-27@20-22-25.mp4\" controls autoplay loop style=\"max-width: 60%;\"  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%manim --hide-splash -qm -v WARNING MarkovByWord\n",
    "\n",
    "layout = gvlayout_factory('dot', heightscale=.5)\n",
    "\n",
    "class MarkovByWord(Scene):\n",
    "  def construct(self):\n",
    "    build_markov_chain(self, corpora['giovanni'], by_char=False, layout = gvlayout_factory('dot', heightscale=.5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
